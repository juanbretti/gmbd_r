---
title: "Programming R Workgroup Project: Machine Learning Model"
author: "Group E"
date: "3/20/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Preparation

## Load libraries

```{r load_libraries, message=FALSE}
# General porpuse
library(tidyverse)
library(data.table)
library(lubridate)

# Descriptive
library(skimr)

# Visualization
library(ggplot2)

# Machine learning
library(e1071)
library(caret)

# Calculations
library(forecast)
library(mice)
library(outliers) 

# Paralel computing
library(foreach)
library(doParallel)
```

# Load data

```{r load_data}
data_solar <- readRDS(file = file.path('data', 'solar_dataset.RData'))
data_station <- fread(file = file.path('data', 'station_info.csv'))
data_add <- readRDS(file = file.path('data', 'additional_variables.RData'))
```

## Transform data

```{r}
# Source dataset
data_solar <- data_solar[j = Date2 := as.Date(x = Date, format = "%Y%m%d")]

# Add date conversions
data_solar <- data_solar %>% 
  mutate(Year = year(Date2),
         Month = month(Date2, label = TRUE),
         Day = day(Date2),
         Day_Of_Year = yday(Date2),
         Day_Of_Week = wday(Date2, label = TRUE, week_start = 1),
         Days_Since_Origin = time_length(interval(origin, Date2), unit = 'day')) %>% 
  as.data.table(.)

# Columns defined from the enunciate
data_solar_col_produ <- colnames(data_solar)[2:99]
data_solar_col_predi <- colnames(data_solar)[100:456]
data_solar_col_dates <- setdiff(colnames(data_solar), c(data_solar_col_produ, data_solar_col_predi))

# Columns defined from the enunciate
data_add_col <- colnames(data_add)[2:101]
data_add_col_dates <- setdiff(colnames(data_add), data_add_col)
```

> principal_weather_station
 [1] "KENT" "BOIS" "HOOK" "HOLL" "GOOD" "TIPT" "BEAV" "ARNE" "CAMA" "CHEY" "ERIC" "MANG" "WOOD" "BUTL" "BUFF" "BESS" "RETR" "SLAP" "HOBA" "FREE" "RING"
[22] "ALTU" "WEAT" "MEDI" "WAUR" "CHER" "APAC" "WATO" "SEIL" "FTCB" "HINT" "PUTN" "ACME" "MAYR" "LAHO" "KETC" "ELRE" "SULP" "NINN" "BREC" "WASH" "MINC"
[43] "BYAR" "CHIC" "GUTH" "FAIR" "SPEN" "MARE" "BURN" "MADI" "SHAW" "REDR" "DURA" "PERK" "PAUL" "MEDF" "ADAX" "NEWK" "PAWN" "HUGO" "BURB" "STIL" "WYNO"
[64] "FORA" "HASK" "BLAC" "CENT" "BOWL" "BRIS" "OILT" "WILB" "CHAN" "NOWA" "BIXB" "OKEM" "LANE" "TISH" "SKIA" "STUA" "COPA" "IDAB" "TAHL" "MIAM" "VINI"
[85] "SALL" "EUFA" "STIG" "MCAL" "JAYX" "COOK" "PRYO" "WIST" "CLOU" "OKMU" "TALI" "CLAY" "WEST" "MTHE"

## Complete data_add

```{r}
data <- select(data_add, !!data_add_col)

m_ <- 5
maxit_ <- 5
# data_mice_ <- mice(data, m=m_, maxit=maxit_, meth='pmm', seed=500)
# saveRDS(data_mice_, file.path('storage', 'data_add_mice.rds'))
data_mice_ <- readRDS(file.path('storage', 'data_add_mice.rds'))
# summary(data_mice_)

# Average of all the Multivariate Imputation
data_mice <- 0
for (i in 1:m_) data_mice <- data_mice + complete(data_mice_, i)
data_mice <- data_mice/m_

data_add_mice <- bind_cols(select(data_add, !!data_add_col_dates), data_mice)

# Cleanup
rm(list = c('data', 'data_mice_', 'm_', 'maxit_', 'i', 'data_mice', 'data_add'))
```

# Join datasets

```{r}
data_solar_add <- data_solar %>% 
  left_join(data_add_mice, by = 'Date', suffix = c(".solar", ".add"))

rm(list = c('data_solar', 'data_add_mice'))
# skim(data_solar_add)
```

# Train, validation, test and predict split

```{r}
data_solar_add_train_ <- data_solar_add[1:5113, ]
data_solar_add_predict <- data_solar_add[5114:nrow(data_solar_add), c(data_solar_col_dates, data_solar_col_predi, data_add_col)]

# row indices for training data (70%)
nrow_train <- round(nrow(data_solar_add_train_)*.7, 0)
# row indices for validation data (15%)
nrow_val <- round(nrow(data_solar_add_train_)*.15, 0)
# row indices for test data (15%)
nrow_test <- nrow(data_solar_add_train_)-nrow_train-nrow_val

data_solar_add_train <- data_solar_add_train_[1:nrow_train, ]
data_solar_add_val <- data_solar_add_train_[(nrow_train+1):(nrow_train+nrow_val), ]
data_solar_add_test <- data_solar_add_train_[(nrow_train+nrow_val+1):nrow(data_solar_add_train_), ]

rm(list=c('nrow_train', 'nrow_val', 'nrow_test', 'data_solar_add_train_', 'data_solar_add', 'data_add_col_dates'))
```

# Variable importance
## Using 'filterVarImp'

```{r}
# cl<-makeCluster(detectCores())
# registerDoParallel(cl)
# 
# select_important<-function(dat, y, n_vars=ncol(dat)){
#   varimp <- filterVarImp(x = dat, y=y, nonpara=TRUE)
#   varimp <- data.table(variable=rownames(varimp),imp=varimp[, 1])
#   varimp <- varimp[order(-imp)]
#   selected <- varimp$variable[1:n_vars]
#   return(selected)
# }
# 
# time_importance <- system.time({
# data_col_importance <- foreach (x = data_solar_col_produ, .errorhandling="remove", .packages=(.packages())) %dopar% {
#                                     select_important(dat=select(data_solar_add_train,
#                                                                 !!c(data_solar_col_predi, 'Day_Of_Year', 'Day_Of_Week', 'Days_Since_Origin', data_add_col)),
#                                                      y = data_solar_add_train[[x]])
#                                     }
# })
# names(data_col_importance) <- data_solar_col_produ
# 
# print(time_importance)
# stopCluster(cl)

# saveRDS(data_col_importance, file.path('storage', 'data_col_importance.rds'))
data_col_importance <- readRDS(file.path('storage', 'data_col_importance.rds'))

rm(list=c('cl', 'select_important', 'time_importance'))
```

# Hyperparameter optimization
## Using class methods

C:\Users\juanb\OneDrive\GMBD\STATISTICAL PROGRAMMING - R (MBD-EN-BL2020J-1_32R369_316435)\Session 16 - Machine Learning\script_s16.R
C:\Users\juanb\OneDrive\GMBD\STATISTICAL PROGRAMMING - R (MBD-EN-BL2020J-1_32R369_316435)\Session 17 - Forum\Ex1\Ex1 Regression.R
https://campus.ie.edu/webapps/discussionboard/do/message?action=list_messages&course_id=_114320331_1&nav=discussion_board_entry&conf_id=_251223_1&forum_id=_112829_1&message_id=_4658342_1


```{r}
### Define grid
cost_values <- 10^seq(from = -2, to = 1, by = 0.5)
gamma_values <- 10^seq(from = -3, to = -1, by = 0.5)
epsilon_values <- 10^seq(from = -2, to = 0, by = 0.5)
```

```{r}
# Parameters
x <- 'KENT'
col_importance_number <- 40

# Subset selection
data_train <- bind_cols(WeatherStation = data_solar_add_train[[x]], 
                  select(data_solar_add_train, data_col_importance[[x]][1:col_importance_number]))
data_val <- bind_cols(WeatherStation = data_solar_add_val[[x]], 
                  select(data_solar_add_val, data_col_importance[[x]][1:col_importance_number]))

# Preprocessing
model_preprocess <- preProcess(x = data_train, method = c('center', 'scale'))
data_preprocess_train <- predict(model_preprocess, data_train)
data_preprocess_val <- predict(model_preprocess, data_val)

### Compute grid search
grid_results <- data.table()

# Parallel grid search
cl<-makeCluster(detectCores())
registerDoParallel(cl)

system.time({
grid_results <- foreach (cost = cost_values, .errorhandling="remove", .combine = rbind) %:%
          foreach (epsilon = epsilon_values, .errorhandling="remove", .combine = rbind) %:%
          foreach (gamma = gamma_values, .errorhandling="remove", .packages=(.packages()), .combine = rbind) %dopar% {
    
            description <- sprintf("cost = %s, gamma = %s, epsilon = %s", cost, gamma, epsilon)
            print(description)
            
            # train SVM model with a particular set of hyperparamets
            model <- svm(WeatherStation ~ ., data = data_train, cost = cost, gamma = gamma, epsilon = epsilon)
            
            # Get model predictions
            predictions_train <- predict(model, newdata = data_train)
            predictions_val <- predict(model, newdata = data_val)
            # Get errors
            errors_train <- predictions_train - data_train$WeatherStation
            errors_val <- predictions_val - data_val$WeatherStation
            # Compute Metrics
            mse_train <- mean(errors_train^2)
            mae_train <- mean(abs(errors_train))
            mse_val <- mean(errors_val^2)
            mae_val <- mean(abs(errors_val))
            
            # Build comparison table
            return(data.table(
              cost = cost,
              epsilon = epsilon,
              gamma = gamma,
              description = description,
              mse_train = mse_train, 
              mae_train = mae_train,
              mse_val = mse_val, 
              mae_val = mae_val))
    }
})

stopCluster(cl)

# Order results by increasing mse and mae
grid_results <- grid_results[order(mse_val, mae_val)]
# Get optimized hyperparameters
best <- grid_results[1]
```

```{r}

### Train final model
# train SVM model with best found set of hyperparamets
model <- svm(WeatherStation ~ ., data = rbind(data_train, data_val), cost = best$cost, epsilon = best$epsilon, gamma = best$gamma)

# Get model predictions
predictions_train <- predict(model, newdata = data_train)
predictions_val <- predict(model, newdata = data_val)
# Get errors
errors_train <- predictions_train - data_train$WeatherStation
errors_val <- predictions_val - data_val$WeatherStation
# Compute Metrics
mse_train <- mean(errors_train^2)
mae_train <- mean(abs(errors_train))
mse_val <- mean(errors_val^2)
mae_val <- mean(abs(errors_val))

ggplot()+
  geom_point(aes(y=predictions_train, x=1:nrow(data_train)), color = 'red') +
  geom_point(aes(y=data_train$WeatherStation, x=1:nrow(data_train)), color = 'blue')
```

## Using caret library

```{r}
# Parameters
x <- 'KENT'
col_importance_number <- 40

# Subset selection
data_train <- bind_cols(WeatherStation = data_solar_add_train[[x]], 
                  select(data_solar_add_train, data_col_importance[[x]][1:col_importance_number]))
data_val <- bind_cols(WeatherStation = data_solar_add_val[[x]], 
                  select(data_solar_add_val, data_col_importance[[x]][1:col_importance_number]))

# Control
fitControl <- trainControl(method = "repeatedcv",   
                           number = 5,     # number of folds
                           repeats = 2,    # repeated ten times
                           search = "random",
                           allowParallel = TRUE)

lambdaGrid <- expand.grid(C = 10^seq(from = -2, to = 1, by = 0.5),
                          sigma = 10^seq(from = -3, to = -1, by = 0.5))

# Parallel grid search
cl<-makeCluster(detectCores())
registerDoParallel(cl)

# Model
system.time({
model_cv <- caret::train(WeatherStation ~ .,
                  data = data_train,
                  method = "svmRadialSigma",
                  metric = "RMSE",
                  tuneLength = 30,
                  trControl = fitControl,
                  preProcess = c('center', 'scale'),
                  tuneGrid = lambdaGrid,
                  na.action = na.omit)
})

stopCluster(cl)

model_cv
```

```{r}
# Get model predictions
predictions_train <- predict(model_cv, newdata = data_train)
predictions_val <- predict(model_cv, newdata = data_val)
# Get errors
errors_train <- predictions_train - data_train$WeatherStation
errors_val <- predictions_val - data_val$WeatherStation
# Compute Metrics
mse_train <- mean(errors_train^2)
mae_train <- mean(abs(errors_train))
mse_val <- mean(errors_val^2)
mae_val <- mean(abs(errors_val))

ggplot()+
  geom_point(aes(y=predictions_train, x=1:nrow(data_train)), color = 'red') +
  geom_point(aes(y=data_train$WeatherStation, x=1:nrow(data_train)), color = 'blue')
```


Alternative
https://rpubs.com/Mai_Thanh_Nguyen/442886